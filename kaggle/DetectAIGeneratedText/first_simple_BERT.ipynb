{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd5c6d47",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "editable": false,
    "execution": {
     "iopub.execute_input": "2024-11-01T03:36:50.630080Z",
     "iopub.status.busy": "2024-11-01T03:36:50.629428Z",
     "iopub.status.idle": "2024-11-01T03:36:51.511335Z",
     "shell.execute_reply": "2024-11-01T03:36:51.509797Z"
    },
    "papermill": {
     "duration": 0.890934,
     "end_time": "2024-11-01T03:36:51.513860",
     "exception": false,
     "start_time": "2024-11-01T03:36:50.622926",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/llm-detect-ai-generated-text/sample_submission.csv\n",
      "/kaggle/input/llm-detect-ai-generated-text/train_prompts.csv\n",
      "/kaggle/input/llm-detect-ai-generated-text/test_essays.csv\n",
      "/kaggle/input/llm-detect-ai-generated-text/train_essays.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5c8912b",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2024-11-01T03:36:51.525405Z",
     "iopub.status.busy": "2024-11-01T03:36:51.524239Z",
     "iopub.status.idle": "2024-11-01T03:36:54.577322Z",
     "shell.execute_reply": "2024-11-01T03:36:54.576248Z"
    },
    "papermill": {
     "duration": 3.06142,
     "end_time": "2024-11-01T03:36:54.579987",
     "exception": false,
     "start_time": "2024-11-01T03:36:51.518567",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "600a6aed",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2024-11-01T03:36:54.591153Z",
     "iopub.status.busy": "2024-11-01T03:36:54.590172Z",
     "iopub.status.idle": "2024-11-01T03:36:54.595118Z",
     "shell.execute_reply": "2024-11-01T03:36:54.594068Z"
    },
    "papermill": {
     "duration": 0.012965,
     "end_time": "2024-11-01T03:36:54.597419",
     "exception": false,
     "start_time": "2024-11-01T03:36:54.584454",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## 预处理 填充序列长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "373c3350",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2024-11-01T03:36:54.607684Z",
     "iopub.status.busy": "2024-11-01T03:36:54.607261Z",
     "iopub.status.idle": "2024-11-01T03:36:54.612039Z",
     "shell.execute_reply": "2024-11-01T03:36:54.610796Z"
    },
    "papermill": {
     "duration": 0.012664,
     "end_time": "2024-11-01T03:36:54.614371",
     "exception": false,
     "start_time": "2024-11-01T03:36:54.601707",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## 词汇表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bfef53c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T03:36:54.625215Z",
     "iopub.status.busy": "2024-11-01T03:36:54.624361Z",
     "iopub.status.idle": "2024-11-01T03:36:54.629228Z",
     "shell.execute_reply": "2024-11-01T03:36:54.628174Z"
    },
    "papermill": {
     "duration": 0.01289,
     "end_time": "2024-11-01T03:36:54.631580",
     "exception": false,
     "start_time": "2024-11-01T03:36:54.618690",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cuda Accelerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8386551",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T03:36:54.642966Z",
     "iopub.status.busy": "2024-11-01T03:36:54.642533Z",
     "iopub.status.idle": "2024-11-01T03:36:54.666481Z",
     "shell.execute_reply": "2024-11-01T03:36:54.665299Z"
    },
    "papermill": {
     "duration": 0.033047,
     "end_time": "2024-11-01T03:36:54.669369",
     "exception": false,
     "start_time": "2024-11-01T03:36:54.636322",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## 模型定义\n",
    "#(batch_size, num_heads, seq_len, d_k)\n",
    "def scaled_dot_product_attention(query, key, value, attention_mask=None):\n",
    "#     print(f'query.shape: {query.shape}, attention_mask.shape: {attention_mask.shape}')\n",
    "    ## todo: q,k,v初始化\n",
    "    d_k = query.size(-1)\n",
    "    # scores:[batch_size, seq_len, seq_len]？\n",
    "    scores = torch.matmul(query, key.transpose(-2,-1)) / math.sqrt(d_k)\n",
    "    if attention_mask is not None:\n",
    "        scores = scores.masked_fill(attention_mask==0, float('-inf'))\n",
    "    attn = F.softmax(scores, dim=-1)\n",
    "    output = torch.matmul(attn, value)\n",
    "    return output, attn\n",
    "    \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.num_heads = num_heads\n",
    "        self.linear_q = nn.Linear(d_model, d_model)\n",
    "        self.linear_k = nn.Linear(d_model, d_model)\n",
    "        self.linear_v = nn.Linear(d_model, d_model)\n",
    "        self.linear_out = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        \n",
    "    def forward(self, query, key, value, attention_mask=None): \n",
    "        batch_size = query.size(0)\n",
    "        # [batch_size, sequence_length, d_model] -> [batch_size, self.num_heads, sequence_length, self.d_k]\n",
    "        q = self.linear_q(query).view(batch_size, -1, self.num_heads, self.d_k).transpose(1,2)\n",
    "        k = self.linear_q(key).view(batch_size, -1, self.num_heads, self.d_k).transpose(1,2)\n",
    "        v = self.linear_q(value).view(batch_size, -1, self.num_heads, self.d_k).transpose(1,2)\n",
    "       \n",
    "        if attention_mask is not None:\n",
    "            attention_mask = attention_mask.unsqueeze(1).unsqueeze(-1)\n",
    "#             print(f'MultiHeadAttention.forward attention_mask unsqueeze: {attention_mask.shape}')\n",
    "        attn_output, _ = scaled_dot_product_attention(q,k,v, attention_mask)\n",
    "        \n",
    "        attn_output = attn_output.transpose(1,2).contiguous().view(batch_size, -1, self.num_heads*self.d_k)\n",
    "        \n",
    "        return self.linear_out(attn_output)\n",
    "\n",
    "    \n",
    "class PositionEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len):\n",
    "        super(PositionEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float()*(-math.log(10000.0)/d_model))\n",
    "        pe[:,0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position*div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0,1)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # [seq_len, batch_size, d_model] + [seq_len, 1, d_model]\n",
    "        return x+self.pe[:x.size(0), :]\n",
    "    \n",
    "    \n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc2(F.relu(self.fc1(x)))\n",
    "    \n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = PositionwiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, x, attention_mask=None): \n",
    "#         print(f'EncoderLayer.forward x.shape:{x.shape}, attention_mask.shape:{attention_mask.shape}')\n",
    "        attn_output = self.attn(x,x,x,attention_mask)\n",
    "        x = self.norm1(x+attn_output)\n",
    "        ffn_output = self.ffn(x)\n",
    "        return self.norm2(x+ffn_output)\n",
    "        \n",
    "\n",
    "# class Encoder(nn.Module):\n",
    "#     def __init__(self, d_model, num_heads, d_ff, src_vocab_size, num_layers, max_len):\n",
    "#         super(Encoder, self).__init__()\n",
    "#         self.embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "#         self.pos_encoding = PositionEncoding(d_model, max_len)\n",
    "#         self.layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff) for _ in range(num_layers)])\n",
    "    \n",
    "#     # src: input_ids\n",
    "#     def forward(self, src):\n",
    "#         x = self.embedding(src) * math.sqrt(self.embedding.embedding_dim)\n",
    "#         x = self.pos_encoding(x)\n",
    "#         for layer in self.layers:\n",
    "#             x = layer(x)\n",
    "            \n",
    "#         return x    \n",
    "       \n",
    "\n",
    "class Detect(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, src_vocab_size, num_layers, max_len, num_labels):\n",
    "        super(Detect, self).__init__()\n",
    "        self.embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.pos_encoding = PositionEncoding(d_model, max_len)\n",
    "        self.encoders = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff) for _ in range(num_layers)])\n",
    "#         self.encoder = Encoder(d_model, num_heads, d_ff, src_vocab_size, num_layers, max_len)\n",
    "        self.fc = nn.Linear(d_model,num_labels)\n",
    "        \n",
    "    def forward(self, src, attention_mask=None):\n",
    "#         print(f'Detect.forward src.shape:{src.shape}, attention_mask.shape:{attention_mask.shape}')\n",
    "        x = self.embedding(src) * math.sqrt(self.embedding.embedding_dim)\n",
    "        x = self.pos_encoding(x)\n",
    "        for encoder in self.encoders:\n",
    "            x = encoder(x, attention_mask)\n",
    "#         x = self.encoder(src)\n",
    "\n",
    "#         print(f'Detect x: {x}')\n",
    "        output = self.fc(x)\n",
    "#         print(f'Detect output: {output.shape}')\n",
    "        return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ceefcb96",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T03:36:54.680205Z",
     "iopub.status.busy": "2024-11-01T03:36:54.679472Z",
     "iopub.status.idle": "2024-11-01T03:36:54.692483Z",
     "shell.execute_reply": "2024-11-01T03:36:54.691406Z"
    },
    "papermill": {
     "duration": 0.021185,
     "end_time": "2024-11-01T03:36:54.694998",
     "exception": false,
     "start_time": "2024-11-01T03:36:54.673813",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# prompt和text 涉及到语言\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# 分词 input_ids+attention_mask\n",
    "class CustomTokenize():\n",
    "    def __init__(self):\n",
    "        self.vocab = {}\n",
    "         # start end符号\n",
    "            \n",
    "        self.vocab['[PAD]'] = 0\n",
    "        self.vocab['[UNK]'] = 1\n",
    "        \n",
    "    # text -> vocab  \n",
    "    def build_vocab(self, text):\n",
    "        words = text.split()\n",
    "        for word in words:\n",
    "            if word not in self.vocab:\n",
    "                self.vocab[word]=len(self.vocab)\n",
    "        \n",
    "    \n",
    "    # text -> words -> input_ids+attention_mask\n",
    "    def __call__(self, text, max_len):\n",
    "        words = text.split()\n",
    "        input_ids = [self.vocab.get(word, self.vocab['[UNK]']) for word in words] \n",
    "        \n",
    "        attention_mask = [1] * len(input_ids)\n",
    "        if len(input_ids) < max_len:\n",
    "            padding_length = max_len-len(input_ids)\n",
    "            input_ids = input_ids + [self.vocab['[PAD]']] * padding_length\n",
    "            attention_mask = attention_mask + [0] * padding_length   \n",
    "        else:\n",
    "            input_ids = input_ids[:max_len]\n",
    "            attention_mask = attention_mask[:max_len]\n",
    "            \n",
    "        ## unsqueeze    \n",
    "#         print(f'CustomTokenize.call input_ids.shape:{torch.tensor(input_ids).shape}')\n",
    "        return {'input_ids': torch.tensor(input_ids), 'attention_mask': torch.tensor(attention_mask)}\n",
    "   \n",
    "\n",
    "# input_ids,attention_mask,label\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_len, labels=None):\n",
    "        if isinstance(texts, pd.Series):\n",
    "            self.texts = texts.tolist()\n",
    "        else:\n",
    "            self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "        if labels is not None:\n",
    "            self.labels = labels\n",
    "        else:\n",
    "            self.labels = None\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        tokenize = self.tokenizer(self.texts[idx], self.max_len)\n",
    "        if self.labels is not None:\n",
    "            label = torch.tensor(self.labels[idx])\n",
    "        else:\n",
    "            label=torch.tensor(-1)\n",
    "        return {'input_ids': tokenize['input_ids'], 'attention_mask': tokenize['attention_mask'], 'label': label}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72036595",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T03:36:54.705334Z",
     "iopub.status.busy": "2024-11-01T03:36:54.704951Z",
     "iopub.status.idle": "2024-11-01T03:36:55.787385Z",
     "shell.execute_reply": "2024-11-01T03:36:55.786000Z"
    },
    "papermill": {
     "duration": 1.090699,
     "end_time": "2024-11-01T03:36:55.790011",
     "exception": false,
     "start_time": "2024-11-01T03:36:54.699312",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "max_len = 256\n",
    "batch_size = 4\n",
    "\n",
    "def combine_text(df):\n",
    "    df['combined_text'] = df[['instructions','source_text','text']].apply(lambda x: ''.join(x.dropna()), axis=1)\n",
    "    return df\n",
    "\n",
    "train_df = pd.read_csv('/kaggle/input/llm-detect-ai-generated-text/train_essays.csv')\n",
    "prompt_df = pd.read_csv('/kaggle/input/llm-detect-ai-generated-text/train_prompts.csv')\n",
    "merge_train = pd.merge(train_df, prompt_df, on='prompt_id')\n",
    "merge_train = combine_text(merge_train)\n",
    "# print(merge_train.head())\n",
    "\n",
    "test_df = pd.read_csv('/kaggle/input/llm-detect-ai-generated-text/test_essays.csv')\n",
    "submission = pd.read_csv('/kaggle/input/llm-detect-ai-generated-text/sample_submission.csv')\n",
    "merge_test = pd.merge(test_df, prompt_df, on='prompt_id',how='left')\n",
    "merge_test = combine_text(merge_test)\n",
    "# print(merge_test.head())\n",
    "\n",
    "tokenizer = CustomTokenize()\n",
    "merge_train['combined_text'].apply(lambda x: tokenizer.build_vocab(x))\n",
    "merge_test['combined_text'].apply(lambda x: tokenizer.build_vocab(x))\n",
    "\n",
    "with open('tokenizer.pkl', 'wb') as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84f43471",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T03:36:55.801207Z",
     "iopub.status.busy": "2024-11-01T03:36:55.800221Z",
     "iopub.status.idle": "2024-11-01T03:36:55.814579Z",
     "shell.execute_reply": "2024-11-01T03:36:55.813469Z"
    },
    "papermill": {
     "duration": 0.022755,
     "end_time": "2024-11-01T03:36:55.817227",
     "exception": false,
     "start_time": "2024-11-01T03:36:55.794472",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('tokenizer.pkl', 'rb') as f:\n",
    "    load_tokenizer = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73b40871",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T03:36:55.827804Z",
     "iopub.status.busy": "2024-11-01T03:36:55.827371Z",
     "iopub.status.idle": "2024-11-01T03:40:10.873366Z",
     "shell.execute_reply": "2024-11-01T03:40:10.872013Z"
    },
    "papermill": {
     "duration": 195.058198,
     "end_time": "2024-11-01T03:40:10.879838",
     "exception": false,
     "start_time": "2024-11-01T03:36:55.821640",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(dataloader): 345\n",
      "epoch 1, loss: 0.02316315148469268\n",
      "epoch 2, loss: 0.015972224736511977\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "## max_len\n",
    "dataset = CustomDataset(merge_train['combined_text'], load_tokenizer, max_len, merge_train['generated'])\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "model = Detect(d_model=128, num_heads=8, d_ff=512, src_vocab_size=len(load_tokenizer.vocab), num_layers=6, max_len=max_len, num_labels=2)\n",
    "# criterion = nn.BCEWithLogitsLoss()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3) ## lr\n",
    "\n",
    "print(f'len(dataloader): {len(dataloader)}')\n",
    "for epoch in range(2):\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        model.train()\n",
    "#         print(batch['attention_mask'].shape)\n",
    "        outputs = model(batch['input_ids'], batch['attention_mask'])\n",
    "        logits = outputs[:,0,:]\n",
    "#         print(f\"logits.shape: {logits.shape}, outputs.shape: {outputs.shape}, batch['input_ids'].shape: {batch['input_ids'].shape}, batch['label'].shape: {batch['label'].shape}\")\n",
    "#         print(f'logits: {logits}')\n",
    "        loss = criterion(logits, batch['label'])\n",
    "        total_loss = total_loss + loss.item()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'epoch {epoch+1}, loss: {total_loss / len(dataloader)}')\n",
    "    \n",
    "## 早停    \n",
    "torch.save(model.state_dict(), 'final_model.pth')    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "251af39b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T03:40:10.890391Z",
     "iopub.status.busy": "2024-11-01T03:40:10.889842Z",
     "iopub.status.idle": "2024-11-01T03:40:10.895881Z",
     "shell.execute_reply": "2024-11-01T03:40:10.894928Z"
    },
    "papermill": {
     "duration": 0.014046,
     "end_time": "2024-11-01T03:40:10.898298",
     "exception": false,
     "start_time": "2024-11-01T03:40:10.884252",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(merge_test['combined_text'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e09d702",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T03:40:10.908791Z",
     "iopub.status.busy": "2024-11-01T03:40:10.908397Z",
     "iopub.status.idle": "2024-11-01T03:40:11.062282Z",
     "shell.execute_reply": "2024-11-01T03:40:11.061119Z"
    },
    "papermill": {
     "duration": 0.161924,
     "end_time": "2024-11-01T03:40:11.064665",
     "exception": false,
     "start_time": "2024-11-01T03:40:10.902741",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17/2750523672.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('final_model.pth'))\n"
     ]
    }
   ],
   "source": [
    "# predict\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "\n",
    "testset = CustomDataset(texts=merge_test['combined_text'], tokenizer=load_tokenizer, max_len=max_len)\n",
    "testloader = DataLoader(testset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "model = Detect(d_model=128, num_heads=8, d_ff=512, src_vocab_size=len(load_tokenizer.vocab), num_layers=6, max_len=max_len, num_labels=2)\n",
    "model.load_state_dict(torch.load('final_model.pth'))\n",
    "# print(model)\n",
    "# for name, param in model.state_dict().items():\n",
    "#     print(f'name: {name}, param: {param.shape}')\n",
    "\n",
    "predicts = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in testloader:\n",
    "        ## todo: test_text过小\n",
    "        output = model(batch['input_ids'], batch['attention_mask'])\n",
    "        first_output = output[:,0,:]\n",
    "#         print(f'output: {first_output}')\n",
    "        predict,_ = torch.max(F.softmax(first_output, dim=-1),dim=1)\n",
    "#         print(f'predict: {predict}')\n",
    "        predicts.append(predict)\n",
    "\n",
    "predicts = torch.cat(predicts)\n",
    "predicts = torch.nan_to_num(predicts, nan=0)\n",
    "# print(f'predicts: {predicts}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f750a43d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T03:40:11.075856Z",
     "iopub.status.busy": "2024-11-01T03:40:11.075426Z",
     "iopub.status.idle": "2024-11-01T03:40:11.099115Z",
     "shell.execute_reply": "2024-11-01T03:40:11.098056Z"
    },
    "papermill": {
     "duration": 0.032173,
     "end_time": "2024-11-01T03:40:11.101501",
     "exception": false,
     "start_time": "2024-11-01T03:40:11.069328",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>generated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000aaaa</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1111bbbb</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2222cccc</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  generated\n",
       "0  0000aaaa        0.0\n",
       "1  1111bbbb        0.0\n",
       "2  2222cccc        0.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_df = pd.DataFrame(predicts, columns=['generated'])    \n",
    "df = pd.concat([test_df, predict_df], axis=1)   \n",
    "\n",
    "submission = df[['id','generated']]    \n",
    "submission.to_csv('/kaggle/working/submission.csv')\n",
    "\n",
    "submission.head()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 7516023,
     "sourceId": 61542,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 204.330025,
   "end_time": "2024-11-01T03:40:12.229849",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-11-01T03:36:47.899824",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
