{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bce5d41",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-04T03:45:29.303477Z",
     "iopub.status.busy": "2024-12-04T03:45:29.302798Z",
     "iopub.status.idle": "2024-12-04T03:45:33.693439Z",
     "shell.execute_reply": "2024-12-04T03:45:33.692772Z"
    },
    "papermill": {
     "duration": 4.398415,
     "end_time": "2024-12-04T03:45:33.695375",
     "exception": false,
     "start_time": "2024-12-04T03:45:29.296960",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "os.environ[\"TRANSFORMERS_OFFLINE\"] = \"1\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6240005b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T03:45:33.703937Z",
     "iopub.status.busy": "2024-12-04T03:45:33.703545Z",
     "iopub.status.idle": "2024-12-04T03:46:17.048528Z",
     "shell.execute_reply": "2024-12-04T03:46:17.047615Z"
    },
    "papermill": {
     "duration": 43.351934,
     "end_time": "2024-12-04T03:46:17.051214",
     "exception": false,
     "start_time": "2024-12-04T03:45:33.699280",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/sci-llm-pip-v2/sentence-transformers-2.2.2.tar.gz\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.6.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (4.45.1)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (4.66.4)\r\n",
      "Requirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (2.4.0)\r\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (0.19.0)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (1.26.4)\r\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (1.2.2)\r\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (1.14.1)\r\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (3.2.4)\r\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (0.2.0)\r\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (0.25.1)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.15.1)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2024.6.1)\r\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (21.3)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (6.0.2)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2.32.3)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (4.12.2)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (1.13.3)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (3.3)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (3.1.4)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (2024.5.15)\r\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (0.4.5)\r\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (0.20.0)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from nltk->sentence-transformers==2.2.2) (1.16.0)\r\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers==2.2.2) (1.4.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers==2.2.2) (3.5.0)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->sentence-transformers==2.2.2) (10.3.0)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.1.2)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->sentence-transformers==2.2.2) (2.1.5)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2024.8.30)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.6.0->sentence-transformers==2.2.2) (1.3.0)\r\n",
      "Building wheels for collected packages: sentence-transformers\r\n",
      "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \bdone\r\n",
      "\u001b[?25h  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125924 sha256=94288921c916a0f14d1fe4b4ede52212da526c93ed2ea1a0450c4341e75ceaaf\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/c6/26/e2/13ef17530724efc5be0bf3d290de1ecaa6c0ff0225fd548014\r\n",
      "Successfully built sentence-transformers\r\n",
      "Installing collected packages: sentence-transformers\r\n",
      "Successfully installed sentence-transformers-2.2.2\r\n"
     ]
    }
   ],
   "source": [
    "!pip install /kaggle/input/sci-llm-pip-v2/sentence-transformers-2.2.2.tar.gz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51360bde",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T03:46:17.062267Z",
     "iopub.status.busy": "2024-12-04T03:46:17.061951Z",
     "iopub.status.idle": "2024-12-04T03:46:17.146287Z",
     "shell.execute_reply": "2024-12-04T03:46:17.145674Z"
    },
    "papermill": {
     "duration": 0.091848,
     "end_time": "2024-12-04T03:46:17.148070",
     "exception": false,
     "start_time": "2024-12-04T03:46:17.056222",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "data_path = Path(\"/kaggle/input/kaggle-llm-science-exam\")\n",
    "\n",
    "test = pd.read_csv(data_path / \"test.csv\")\n",
    "CALC_SCORE = False\n",
    "\n",
    "test.head()\n",
    "test.to_parquet(\"test_raw.pq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04319a53",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T03:46:17.159615Z",
     "iopub.status.busy": "2024-12-04T03:46:17.159322Z",
     "iopub.status.idle": "2024-12-04T03:46:17.168782Z",
     "shell.execute_reply": "2024-12-04T03:46:17.168002Z"
    },
    "papermill": {
     "duration": 0.017568,
     "end_time": "2024-12-04T03:46:17.170364",
     "exception": false,
     "start_time": "2024-12-04T03:46:17.152796",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing get_topk.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile get_topk.py\n",
    "\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import torch\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pathlib import Path\n",
    "from joblib import Parallel, delayed\n",
    "import argparse\n",
    "\n",
    "\n",
    "def cos_similarity_matrix(a: torch.Tensor, b: torch.Tensor):\n",
    "    \"\"\"Calculates cosine similarities between tensor a and b.\"\"\"\n",
    "\n",
    "    sim_mt = torch.mm(a, b.transpose(0, 1))\n",
    "    return sim_mt\n",
    "\n",
    "\n",
    "def get_topk(embeddings_from, embeddings_to, topk=1000, bs=512):\n",
    "    chunk = bs\n",
    "    embeddings_chunks = embeddings_from.split(chunk)\n",
    "\n",
    "    vals = []\n",
    "    inds = []\n",
    "    for idx in range(len(embeddings_chunks)):\n",
    "        cos_sim_chunk = cos_similarity_matrix(\n",
    "            embeddings_chunks[idx].to(embeddings_to.device).half(), embeddings_to\n",
    "        ).float()\n",
    "\n",
    "        cos_sim_chunk = torch.nan_to_num(cos_sim_chunk, nan=0.0)\n",
    "\n",
    "        topk = min(topk, cos_sim_chunk.size(1))\n",
    "        vals_chunk, inds_chunk = torch.topk(cos_sim_chunk, k=topk, dim=1)\n",
    "        vals.append(vals_chunk[:, :].detach().cpu())\n",
    "        inds.append(inds_chunk[:, :].detach().cpu())\n",
    "\n",
    "        del vals_chunk\n",
    "        del inds_chunk\n",
    "        del cos_sim_chunk\n",
    "\n",
    "    vals = torch.cat(vals).detach().cpu()\n",
    "    inds = torch.cat(inds).detach().cpu()\n",
    "\n",
    "    return inds, vals\n",
    "\n",
    "\n",
    "def insert_value_at(tensor, value, position):\n",
    "    # Ensure the position is valid\n",
    "    if position < 0 or position >= len(tensor):\n",
    "        raise ValueError(\"Position should be between 0 and tensor length - 1.\")\n",
    "\n",
    "    # Slice the tensor into two parts\n",
    "    left = tensor[:position]\n",
    "    right = tensor[position:]\n",
    "\n",
    "    # Create a tensor for the value to be inserted\n",
    "    value_tensor = torch.tensor([value], dtype=tensor.dtype)\n",
    "\n",
    "    # Concatenate the tensors together and slice to the original length\n",
    "    result = torch.cat([left, value_tensor, right])[:-1]\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def insert_value_at_list(lst, value, position):\n",
    "    # Ensure the position is valid\n",
    "    if position < 0 or position >= len(lst):\n",
    "        raise ValueError(\"Position should be between 0 and list length - 1.\")\n",
    "\n",
    "    # Insert value at the specified position\n",
    "    lst.insert(position, value)\n",
    "\n",
    "    # Remove the last value to maintain original length\n",
    "    lst.pop()\n",
    "\n",
    "    return lst\n",
    "\n",
    "\n",
    "def remove_consecutive_duplicates(input_list):\n",
    "    if not input_list:\n",
    "        return [\" \"] * args.topk\n",
    "\n",
    "    new_list = [input_list[0]]\n",
    "    for i in range(1, len(input_list)):\n",
    "        if input_list[i] != input_list[i - 1]:\n",
    "            new_list.append(input_list[i])\n",
    "\n",
    "    # Append empty strings if new_list length is less than 5\n",
    "    while len(new_list) < args.topk:\n",
    "        new_list.append(\" \")\n",
    "\n",
    "    return new_list\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    ap = argparse.ArgumentParser()\n",
    "    ap.add_argument(\"--wiki\", type=str, required=True)\n",
    "    ap.add_argument(\"--model_name\", type=str, required=True)\n",
    "    ap.add_argument(\"--test_file\", type=str, required=True)\n",
    "    ap.add_argument(\"--topk\", type=int, required=True)\n",
    "    ap.add_argument(\"--ind\", type=int, required=True)\n",
    "    args = ap.parse_args()\n",
    "\n",
    "    if args.topk == 10:\n",
    "        TOP_K = 20\n",
    "    else:\n",
    "        TOP_K = 10\n",
    "\n",
    "    data_path = Path(\"/kaggle/input/kaggle-llm-science-exam\")\n",
    "\n",
    "    if args.wiki == \"cirrus\":\n",
    "        files_all = sorted(list(glob(\"/kaggle/input/cirruswiki-titles/*.parquet\")))\n",
    "    elif args.wiki == \"new\":\n",
    "        files_all = sorted(list(glob(\"/kaggle/input/newwiki-titles/*.parquet\")))\n",
    "\n",
    "    if \"e5-large\" in args.model_name:\n",
    "        files_np = sorted(\n",
    "            list(glob(\"/kaggle/input/enwiki-cirrus-20230701-e5-large-part*/*.npy\"))\n",
    "        )\n",
    "    elif \"gte-large\" in args.model_name:\n",
    "        files_np = sorted(\n",
    "            list(glob(\"/kaggle/input/wiki31m-gte-large-title-p*/*.npy\"))\n",
    "        )\n",
    "\n",
    "    files_all = [(x, y) for x, y in zip(files_all, files_np)]\n",
    "    files = [files_all[: len(files_all) // 2], files_all[len(files_all) // 2 :]]\n",
    "\n",
    "    if \"e5-large\" in args.model_name:\n",
    "        model = SentenceTransformer(\"/kaggle/input/intfloat-e5-large-v2\").to(\"cuda:0\")\n",
    "    elif \"gte-large\" in args.model_name:\n",
    "        model = SentenceTransformer(\"/kaggle/input/thenlper-gte-large\").to(\"cuda:0\")\n",
    "\n",
    "    test = pd.read_parquet(\"test_raw.pq\")\n",
    "\n",
    "    embs = []\n",
    "    for idx, row in test.iterrows():\n",
    "        if \"e5\" in args.model_name:\n",
    "            sentences = [\n",
    "                \"query: \"\n",
    "                + row.prompt\n",
    "                + \" \"\n",
    "                + row.A\n",
    "                + \" \"\n",
    "                + row.B\n",
    "                + \" \"\n",
    "                + row.C\n",
    "                + \" \"\n",
    "                + row.D\n",
    "                + \" \"\n",
    "                + row.E\n",
    "            ]\n",
    "        elif \"gte\" in args.model_name:\n",
    "            sentences = [\n",
    "                row.prompt\n",
    "                + \" \"\n",
    "                + row.A\n",
    "                + \" \"\n",
    "                + row.B\n",
    "                + \" \"\n",
    "                + row.C\n",
    "                + \" \"\n",
    "                + row.D\n",
    "                + \" \"\n",
    "                + row.E\n",
    "            ]\n",
    "\n",
    "        embeddings = torch.Tensor(\n",
    "            model.encode(sentences, show_progress_bar=False, normalize_embeddings=True)\n",
    "        )\n",
    "        embs.append(torch.nn.functional.normalize(embeddings, dim=1))\n",
    "\n",
    "    query_embeddings = torch.Tensor(np.stack(embs)).squeeze(1)\n",
    "    print(f'query_embeddings.shape: {query_embeddings.shape}')\n",
    "\n",
    "    # Create placeholders for top-k matches\n",
    "    all_vals_gpu_0 = torch.full((len(test), TOP_K), -float(\"inf\"), dtype=torch.float16)\n",
    "    all_texts_gpu_0 = [[None] * TOP_K for _ in range(len(all_vals_gpu_0))]\n",
    "\n",
    "    all_vals_gpu_1 = torch.full((len(test), TOP_K), -float(\"inf\"), dtype=torch.float16)\n",
    "    all_texts_gpu_1 = [[None] * TOP_K for _ in range(len(all_vals_gpu_1))]\n",
    "\n",
    "    def load_data(files, device):\n",
    "        for file, file_np in files:\n",
    "            df = pd.read_parquet(file, engine=\"pyarrow\", use_threads=True)\n",
    "            file_embeddings = np.load(file_np)\n",
    "\n",
    "            data_embeddings = torch.Tensor(file_embeddings).to(device).half()\n",
    "            data_embeddings = torch.nn.functional.normalize(data_embeddings, dim=1)\n",
    "\n",
    "            max_inds, max_vals = get_topk(\n",
    "                query_embeddings, data_embeddings, topk=TOP_K, bs=8\n",
    "            )\n",
    "\n",
    "            # loop through all queries (test)\n",
    "            for i in range(len(test)):\n",
    "                # start with highest new val (pos 0) vs worst value already in the toplist (pos topk - 1)\n",
    "                for new in range(TOP_K):\n",
    "                    if device == \"cuda:0\":\n",
    "                        if max_vals[i][new].item() < all_vals_gpu_0[i][TOP_K - 1]:\n",
    "                            break\n",
    "                        for old in range(TOP_K):\n",
    "                            if max_vals[i][new].item() > all_vals_gpu_0[i][old]:\n",
    "                                all_vals_gpu_0[i] = insert_value_at(\n",
    "                                    all_vals_gpu_0[i],\n",
    "                                    value=max_vals[i][new].item(),\n",
    "                                    position=old,\n",
    "                                )\n",
    "                                all_texts_gpu_0[i] = insert_value_at_list(\n",
    "                                    all_texts_gpu_0[i],\n",
    "                                    value=df.iloc[max_inds[i][new].item()].text,\n",
    "                                    position=old,\n",
    "                                )\n",
    "                                break\n",
    "                    else:\n",
    "                        if max_vals[i][new].item() < all_vals_gpu_1[i][TOP_K - 1]:\n",
    "                            break\n",
    "                        for old in range(TOP_K):\n",
    "                            if max_vals[i][new].item() > all_vals_gpu_1[i][old]:\n",
    "                                all_vals_gpu_1[i] = insert_value_at(\n",
    "                                    all_vals_gpu_1[i],\n",
    "                                    value=max_vals[i][new].item(),\n",
    "                                    position=old,\n",
    "                                )\n",
    "                                all_texts_gpu_1[i] = insert_value_at_list(\n",
    "                                    all_texts_gpu_1[i],\n",
    "                                    value=df.iloc[max_inds[i][new].item()].text,\n",
    "                                    position=old,\n",
    "                                )\n",
    "                                break\n",
    "\n",
    "    Parallel(n_jobs=2, backend=\"threading\")(\n",
    "        delayed(load_data)(files[i], f\"cuda:{i}\") for i in range(2)\n",
    "    )\n",
    "    all_vals = torch.hstack([all_vals_gpu_0, all_vals_gpu_1])\n",
    "    val, inds = torch.topk(all_vals.float(), axis=1, k=TOP_K)\n",
    "    all_texts = [\n",
    "        [(t0 + t1)[inner_idx.item()] for inner_idx in idx]\n",
    "        for t0, t1, idx in zip(all_texts_gpu_0, all_texts_gpu_1, inds)\n",
    "    ]\n",
    "\n",
    "    all_texts = [remove_consecutive_duplicates(lst) for lst in all_texts]\n",
    "\n",
    "    test[\"context\"] = [\n",
    "        \"\\n###\\n\".join([x[i] for i in list(range(args.topk))[::-1]]) for x in all_texts\n",
    "    ]\n",
    "\n",
    "    test[\"context_v2\"] = [\n",
    "        \"Context 4: \"\n",
    "        + x[4]\n",
    "        + \"\\n###\\n\"\n",
    "        + \"Context 3: \"\n",
    "        + x[3]\n",
    "        + \"\\n###\\n\"\n",
    "        + \"Context 2: \"\n",
    "        + x[2]\n",
    "        + \"\\n###\\n\"\n",
    "        + \"Context 1: \"\n",
    "        + x[1]\n",
    "        + \"\\n###\\n\"\n",
    "        + \"Context 0: \"\n",
    "        + x[0]\n",
    "        for x in all_texts\n",
    "    ]\n",
    "\n",
    "    print(test[\"context\"].values[0])\n",
    "\n",
    "    test.to_parquet(f\"{args.test_file}.pq\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b32bfcf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T03:46:17.180192Z",
     "iopub.status.busy": "2024-12-04T03:46:17.179737Z",
     "iopub.status.idle": "2024-12-04T03:46:17.184040Z",
     "shell.execute_reply": "2024-12-04T03:46:17.183319Z"
    },
    "papermill": {
     "duration": 0.010857,
     "end_time": "2024-12-04T03:46:17.185544",
     "exception": false,
     "start_time": "2024-12-04T03:46:17.174687",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing run.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile run.sh\n",
    "\n",
    "python get_topk.py --wiki \"cirrus\" --model_name \"e5-large\" --test_file \"test\" --topk 5 --ind 0  &&\n",
    "python get_topk.py --wiki \"new\" --model_name \"gte-large\" --test_file \"test2\" --topk 5 --ind 0  &&\n",
    "\n",
    "wait \n",
    "echo \"All done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c77466d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T03:46:17.195095Z",
     "iopub.status.busy": "2024-12-04T03:46:17.194879Z",
     "iopub.status.idle": "2024-12-04T04:12:41.178872Z",
     "shell.execute_reply": "2024-12-04T04:12:41.177785Z"
    },
    "papermill": {
     "duration": 1583.990909,
     "end_time": "2024-12-04T04:12:41.180887",
     "exception": false,
     "start_time": "2024-12-04T03:46:17.189978",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_embeddings.shape: torch.Size([200, 1024])\r\n",
      "Physical Review Letters. 99 (14): 141302. arXiv:0704.1932. Bibcode:2007PhRvL..99n1302Z. doi:10.1103/PhysRevLett.99.141302. PMID 17930657. S2CID 119672184. Alzain, Mohammed (2017). \"Modified Newtonian Dynamics (MOND) as a Modification of Newtonian Inertia\". Journal of Astrophysics and Astronomy. 38 (4): 59. arXiv:1708.05385. Bibcode:2017JApA...38...59A. doi:10.1007/s12036-017-9479-0. S2CID 119245210. S. McGaugh, The EFE in MOND Archived 2017-07-16 at the Wayback Machine Milgrom, Mordehai (2008).\r\n",
      "###\r\n",
      "The homogeneously distributed mass of the universe would result in a roughly scalar field that permeated the universe and would serve as a source for Newton's gravitational constant; creating a theory of quantum gravity. Modified Newtonian Dynamics (MOND) is a relatively modern proposal to explain the galaxy rotation problem based on a variation of Newton's Second Law of Dynamics at low accelerations.\r\n",
      "###\r\n",
      "Modified Newtonian dynamics (MOND) is a hypothesis that proposes a modification of Newton's law of universal gravitation to account for observed properties of galaxies. It is an alternative to the hypothesis of dark matter in terms of explaining why galaxies do not appear to obey the currently understood laws of physics. Created in 1982 and first published in 1983 by Israeli physicist Mordehai Milgrom, the hypothesis' original motivation was to explain why the velocities of stars in galaxies were observed to be larger than expected based on Newtonian mechanics.\r\n",
      "###\r\n",
      "Unsolved problem in physics: What is the nature of dark matter? Is it a particle, or do the phenomena attributed to dark matter actually require a modification of the laws of gravity? (more unsolved problems in physics) MOND is an example of a class of theories known as modified gravity, and is an alternative to the hypothesis that the dynamics of galaxies are determined by massive, invisible dark matter halos.\r\n",
      "###\r\n",
      "MOND reduces the discrepancy between the velocity dispersions and clusters' observed missing baryonic mass from a factor of around 10 to a factor of about 2. However, the residual discrepancy cannot be accounted for by MOND, requiring that other explanations close the gap such as the presence of as-yet undetected missing baryonic matter.\r\n",
      "query_embeddings.shape: torch.Size([200, 1024])\r\n",
      "Despite its vanishingly small and undetectable effects on bodies that are on Earth, within the Solar System, and even in proximity to the Solar System and other planetary systems, MOND successfully explains significant observed galactic-scale rotational effects without invoking the existence of as-yet undetected dark matter particles lying outside of the highly successful Standard Model of particle physics. This is in large part due to MOND holding that exceedingly weak galactic-scale gravity holding galaxies together near their perimeters declines as a very slow linear relationship to distance from the center of a galaxy rather than declining as the inverse square of distance.\r\n",
      "###\r\n",
      "The majority of astronomers, astrophysicists, and cosmologists accept dark matter as the explanation for galactic rotation curves (based on general relativity, and hence Newtonian mechanics), and are committed to a dark matter solution of the missing-mass problem. The primary difference between supporters of ΛCDM and MOND is in the observations for which they demand a robust, quantitative explanation, and those for which they are satisfied with a qualitative account, or are prepared to leave for future work. Proponents of MOND emphasize predictions made on galaxy scales (where MOND enjoys its most notable successes) and believe that a cosmological model consistent with galaxy dynamics has yet to be discovered. Proponents of ΛCDM require high levels of cosmological accuracy (which concordance cosmology provides) and argue that a resolution of galaxy-scale issues will follow from a better understanding of the complicated baryonic astrophysics underlying galaxy formation.\r\n",
      "###\r\n",
      "Hypothesis proposing a modification of Newton's laws Modified Newtonian dynamics (MOND) is a hypothesis that proposes a modification of Newton's law of universal gravitation to account for observed properties of galaxies. It is an alternative to the hypothesis of dark matter in terms of explaining why galaxies do not appear to obey the currently understood laws of physics.\r\n",
      "###\r\n",
      "&lt;templatestyles src=\"Unsolved/styles.css\" /&gt; MOND is an example of a class of theories known as modified gravity, and is an alternative to the hypothesis that the dynamics of galaxies are determined by massive, invisible dark matter halos. Since Milgrom's original proposal, proponents of MOND have claimed to successfully predict a variety of galactic phenomena that they state are difficult to understand as consequences of dark matter.\r\n",
      "###\r\n",
      "Though MOND explains the anomalously great rotational velocities of galaxies at their perimeters, it does not fully explain the velocity dispersions of individual galaxies within galaxy clusters. MOND reduces the discrepancy between the velocity dispersions and clusters' observed missing baryonic mass from a factor of around 10 to a factor of about 2. However, the residual discrepancy cannot be accounted for by MOND, requiring that other explanations close the gap such as the presence of as-yet undetected missing baryonic matter.\r\n",
      "All done\r\n"
     ]
    }
   ],
   "source": [
    "!sh run.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0fccb83c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T04:12:41.192283Z",
     "iopub.status.busy": "2024-12-04T04:12:41.191975Z",
     "iopub.status.idle": "2024-12-04T04:13:43.403475Z",
     "shell.execute_reply": "2024-12-04T04:13:43.402405Z"
    },
    "papermill": {
     "duration": 62.219759,
     "end_time": "2024-12-04T04:13:43.405775",
     "exception": false,
     "start_time": "2024-12-04T04:12:41.186016",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/sci-llm-pip-v2/bitsandbytes-0.41.0-py3-none-any.whl\r\n",
      "Installing collected packages: bitsandbytes\r\n",
      "Successfully installed bitsandbytes-0.41.0\r\n",
      "Looking in links: /kaggle/input/transformers-main\r\n",
      "Processing /kaggle/input/transformers-main/transformers-4.46.3-py3-none-any.whl\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.46.3) (3.15.1)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers==4.46.3) (0.25.1)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.46.3) (1.26.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.46.3) (21.3)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.46.3) (6.0.2)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.46.3) (2024.5.15)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.46.3) (2.32.3)\r\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers==4.46.3) (0.20.0)\r\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.46.3) (0.4.5)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.46.3) (4.66.4)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.46.3) (2024.6.1)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.46.3) (4.12.2)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers==4.46.3) (3.1.2)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.46.3) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.46.3) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.46.3) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.46.3) (2024.8.30)\r\n",
      "Installing collected packages: transformers\r\n",
      "  Attempting uninstall: transformers\r\n",
      "    Found existing installation: transformers 4.45.1\r\n",
      "    Uninstalling transformers-4.45.1:\r\n",
      "      Successfully uninstalled transformers-4.45.1\r\n",
      "Successfully installed transformers-4.46.3\r\n"
     ]
    }
   ],
   "source": [
    "!pip install /kaggle/input/sci-llm-pip-v2/bitsandbytes-0.41.0-py3-none-any.whl\n",
    "!pip install --no-index --find-links=\"/kaggle/input/transformers-main\" /kaggle/input/transformers-main/transformers-4.46.3-py3-none-any.whl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf75881b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T04:13:43.418868Z",
     "iopub.status.busy": "2024-12-04T04:13:43.418514Z",
     "iopub.status.idle": "2024-12-04T04:13:43.428499Z",
     "shell.execute_reply": "2024-12-04T04:13:43.427488Z"
    },
    "papermill": {
     "duration": 0.018568,
     "end_time": "2024-12-04T04:13:43.430209",
     "exception": false,
     "start_time": "2024-12-04T04:13:43.411641",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing inference.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile inference.py\n",
    "\n",
    "# zero-shot: preprompt + instructions\n",
    "import sys\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import gc\n",
    "import itertools\n",
    "\n",
    "def longest_common_prefix(strs):\n",
    "    if not strs:\n",
    "        return \"\"\n",
    "\n",
    "    shortest = min(strs, key=len)\n",
    "\n",
    "    for i, char in enumerate(shortest):\n",
    "        for other in strs:\n",
    "            if other[i] != char:\n",
    "                return shortest[:i]\n",
    "\n",
    "    return shortest\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    ap = argparse.ArgumentParser()\n",
    "    ap.add_argument(\"--device\", type=str, required=True)\n",
    "    ap.add_argument(\"--model_name\", type=str, required=True)\n",
    "    ap.add_argument(\"--quantization\", type=int, required=True)\n",
    "    ap.add_argument(\"--model_type\", type=int, required=True)\n",
    "    ap.add_argument(\"--test_file\", type=str, required=True)\n",
    "    args = ap.parse_args()\n",
    "\n",
    "    if args.device == \"auto\":\n",
    "        DEVICE_MAP = \"auto\"\n",
    "        DEVICE = \"cuda:0\"\n",
    "    else:\n",
    "        DEVICE_MAP = {\"\": args.device}\n",
    "        DEVICE = args.device\n",
    "\n",
    "    llm_backbone = args.model_name\n",
    "\n",
    "    test = pd.read_parquet(args.test_file).reset_index(drop=True)\n",
    "\n",
    "    new_obs = []\n",
    "    for idx, row in test.iterrows():\n",
    "        for opt in \"ABCDE\":\n",
    "            new_obs.append(\n",
    "                (row[\"id\"], row[\"context\"], row[\"context_v2\"], row[\"prompt\"], row[opt])\n",
    "            )\n",
    "    df = pd.DataFrame(\n",
    "        new_obs, columns=[\"id\", \"context\", \"context_v2\", \"question\", \"answer\"]\n",
    "    )\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        llm_backbone,\n",
    "        use_fast=True,\n",
    "        trust_remote_code=True,\n",
    "        padding_side=\"right\",\n",
    "        truncation_side=\"left\",\n",
    "    )\n",
    "\n",
    "    if tokenizer.pad_token is None:\n",
    "        if tokenizer.unk_token is not None:\n",
    "            tokenizer.pad_token = tokenizer.unk_token\n",
    "        else:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    from transformers import BitsAndBytesConfig\n",
    "\n",
    "    if args.quantization == 0:\n",
    "        quantization_config = None\n",
    "    elif args.quantization == 1:\n",
    "        quantization_config = BitsAndBytesConfig(\n",
    "            load_in_8bit=True, llm_int8_threshold=0.0\n",
    "        )\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        llm_backbone,\n",
    "        torch_dtype=torch.float16,\n",
    "        quantization_config=quantization_config,\n",
    "        device_map=DEVICE_MAP,\n",
    "        low_cpu_mem_usage=True,\n",
    "        trust_remote_code=True,\n",
    "    ).eval()\n",
    "\n",
    "    # head_weights = torch.load(llm_backbone + \"/head.pth\", map_location=\"cpu\")\n",
    "    # hidden_size = head_weights.shape[1] # 64004\n",
    "\n",
    "    # print('hidden_size: ', hidden_size)\n",
    "    # head = torch.nn.Linear(hidden_size, 1, bias=False)\n",
    "\n",
    "    # head.weight.data = head_weights\n",
    "\n",
    "    hidden_size = 64004\n",
    "    head = torch.nn.Linear(hidden_size, 1, bias=False)\n",
    "\n",
    "    head.to(DEVICE).eval()\n",
    "\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    progress_bar = tqdm(df.iterrows(), total=len(df))\n",
    "\n",
    "    preds = []\n",
    "    instructions = []\n",
    "    pooled = []\n",
    "    past_key_values = None\n",
    "    for idx, row in progress_bar:\n",
    "        inst = f\"Answer: {row['answer']}\\n###\\nIs this answer correct? \"\n",
    "        instructions.append(inst)\n",
    "\n",
    "        if idx % 5 == 0:\n",
    "            if past_key_values is not None:\n",
    "                del past_key_values\n",
    "\n",
    "            preprompt = f\"{row['context_v2']}\\n###\\nQuestion: {row['question']}\\n###\\n\"\n",
    "            inputs = tokenizer(\n",
    "                preprompt,\n",
    "                return_tensors=\"pt\",\n",
    "                add_special_tokens=False,\n",
    "                truncation=True,\n",
    "                padding=\"longest\",\n",
    "                max_length=1024,\n",
    "            )\n",
    "\n",
    "            tok_length = (\n",
    "                inputs[\"input_ids\"].shape[1]\n",
    "                + tokenizer(\n",
    "                    instructions,\n",
    "                    return_tensors=\"pt\",\n",
    "                    add_special_tokens=False,\n",
    "                    truncation=True,\n",
    "                    padding=\"longest\",\n",
    "                    max_length=1024,\n",
    "                )[\"input_ids\"].shape[1]\n",
    "            )\n",
    "\n",
    "            BATCH_SIZE = 5 # ABCDE\n",
    "\n",
    "            with torch.no_grad():\n",
    "                past_key_values = list(\n",
    "                    model(input_ids=inputs[\"input_ids\"].to(DEVICE)).past_key_values\n",
    "                )\n",
    "\n",
    "                for idx0 in range(len(past_key_values)):\n",
    "                    past_key_values[idx0] = list(past_key_values[idx0])\n",
    "                    for idx1 in range(len(past_key_values[idx0])):\n",
    "                        past_key_values[idx0][idx1] = past_key_values[idx0][\n",
    "                            idx1\n",
    "                        ].expand(BATCH_SIZE, -1, -1, -1)\n",
    "            del inputs\n",
    "\n",
    "        if (idx + 1) % BATCH_SIZE == 0 or idx == len(df) - 1:\n",
    "            inputs = tokenizer(\n",
    "                instructions,\n",
    "                return_tensors=\"pt\",\n",
    "                add_special_tokens=False,\n",
    "                truncation=True,\n",
    "                padding=\"longest\",\n",
    "            )\n",
    "\n",
    "            with torch.no_grad():\n",
    "                out = model(\n",
    "                    input_ids=inputs[\"input_ids\"].to(DEVICE),\n",
    "                    past_key_values=past_key_values,\n",
    "                ).logits #[5,57,32002]\n",
    "\n",
    "\n",
    "                # ## ----------------display\n",
    "                # att_idx = inputs[\"attention_mask\"].sum(dim=1)[0] - 1\n",
    "                # logit = out[3,att_idx,:]\n",
    "                # soft_logit = torch.softmax(logit, dim=-1)\n",
    "                # max_index = torch.argmax(soft_logit).item()\n",
    "                # vocab = tokenizer.get_vocab() #[token:id]\n",
    "                # id_to_token = {v:k for k,v in vocab.items()}\n",
    "                # print('max_value:', id_to_token[max_index])\n",
    "\n",
    "                # with open('id_to_token.txt', 'w') as f:\n",
    "                #     for k, v in id_to_token.items():\n",
    "                #         f.write(f'id: {k}, token: {v}\\n')\n",
    "\n",
    "                # with open('logit.txt', 'w') as f:\n",
    "                #     for i, v in enumerate(soft_logit):\n",
    "                #         f.write(f'id: {i}, prob: {v}\\n')\n",
    "                # sys.exit()\n",
    "                # # --------------------\n",
    "                \n",
    "                for jjj in range(len(out)):\n",
    "                    att_idx = inputs[\"attention_mask\"].sum(dim=1)[jjj] - 1\n",
    "                    pooled.append(out[jjj, att_idx, :].float().unsqueeze(0)) \n",
    "\n",
    "                # pooled:batch_size个[1, 32002]\n",
    "\n",
    "            instructions = []\n",
    "            del out\n",
    "            del inputs\n",
    "\n",
    "        if (idx + 1) % 5 == 0:\n",
    "            with torch.no_grad():\n",
    "                pooled = torch.cat(pooled) # [5, 32002]\n",
    "\n",
    "                new_poolings = []\n",
    "\n",
    "                # 当前答案嵌入与其余答案嵌入均值拼接\n",
    "                indexes = np.arange(0, 5)\n",
    "                for jj in indexes:\n",
    "                    other_embeddings = pooled[\n",
    "                        [jjj for jjj in indexes if jjj != jj]\n",
    "                    ]\n",
    "                    new_poolings.append(\n",
    "                        torch.cat([pooled[jj], torch.mean(other_embeddings, dim=0)])\n",
    "                    ) # 5个[64004]\n",
    "\n",
    "                new_poolings = torch.stack(new_poolings) # [5, 64004]\n",
    "\n",
    "                logits = head(new_poolings) # [5,1] logits=new_poolings×W⊤\n",
    "                logits = logits[:, 0] # [5] \n",
    "                logits = logits.detach().cpu().numpy()\n",
    "\n",
    "                for lg in logits:\n",
    "                    preds.append(lg)\n",
    "\n",
    "            del logits\n",
    "            pooled = []\n",
    "\n",
    "    np.save(f\"scores_{llm_backbone.split('/')[-1]}_{args.test_file.split('/')[-1][:-3]}\", preds)\n",
    "\n",
    "\n",
    "# 最后一个token的每个vocab概率 -> 答案正确率\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d3b6d7c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T04:13:43.442676Z",
     "iopub.status.busy": "2024-12-04T04:13:43.442359Z",
     "iopub.status.idle": "2024-12-04T04:13:43.448007Z",
     "shell.execute_reply": "2024-12-04T04:13:43.447176Z"
    },
    "papermill": {
     "duration": 0.013727,
     "end_time": "2024-12-04T04:13:43.449660",
     "exception": false,
     "start_time": "2024-12-04T04:13:43.435933",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting run.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile run.sh\n",
    "\n",
    "# python inference.py --device \"auto\" --model_name \"/kaggle/input/openorca-mistral-7b-openorca\" --quantization 0 --model_type 1 --test_file \"/kaggle/input/llmse-zeroshot-gettopk/test.pq\"  &&\n",
    "# python inference.py --device \"auto\" --model_name \"/kaggle/input/openorca-mistral-7b-openorca\" --quantization 0 --model_type 1 --test_file \"/kaggle/input/llmse-zeroshot-gettopk/test2.pq\"  &&\n",
    "\n",
    "python inference.py --device \"auto\" --model_name \"/kaggle/input/openorca-mistral-7b-openorca\" --quantization 0 --model_type 1 --test_file \"test.pq\"  &&\n",
    "python inference.py --device \"auto\" --model_name \"/kaggle/input/openorca-mistral-7b-openorca\" --quantization 0 --model_type 1 --test_file \"test2.pq\"  &&\n",
    "\n",
    "wait \n",
    "echo \"All done\"\n",
    "\n",
    "# /kaggle/input/teamhydrogen-white-malamute-prompt-openorca-v2 head预训练过    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "211ef8ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T04:13:43.462135Z",
     "iopub.status.busy": "2024-12-04T04:13:43.461896Z",
     "iopub.status.idle": "2024-12-04T04:21:42.146020Z",
     "shell.execute_reply": "2024-12-04T04:21:42.144756Z"
    },
    "papermill": {
     "duration": 478.692662,
     "end_time": "2024-12-04T04:21:42.148038",
     "exception": false,
     "start_time": "2024-12-04T04:13:43.455376",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [01:45<00:00, 52.60s/it]\r\n",
      "  0%|                                          | 1/1000 [00:00<15:00,  1.11it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\r\n",
      "We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)\r\n",
      "100%|███████████████████████████████████████| 1000/1000 [02:32<00:00,  6.56it/s]\r\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:16<00:00,  8.33s/it]\r\n",
      "  0%|                                          | 1/1000 [00:00<13:34,  1.23it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\r\n",
      "We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)\r\n",
      "100%|███████████████████████████████████████| 1000/1000 [02:55<00:00,  5.69it/s]\r\n",
      "All done\r\n"
     ]
    }
   ],
   "source": [
    "!sh run.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b474829",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T04:21:42.195495Z",
     "iopub.status.busy": "2024-12-04T04:21:42.195154Z",
     "iopub.status.idle": "2024-12-04T04:21:42.355733Z",
     "shell.execute_reply": "2024-12-04T04:21:42.354654Z"
    },
    "papermill": {
     "duration": 0.186201,
     "end_time": "2024-12-04T04:21:42.357479",
     "exception": false,
     "start_time": "2024-12-04T04:21:42.171278",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores_openorca-mistral-7b-openorca_test.npy\n",
      "0.2\n",
      "scores_openorca-mistral-7b-openorca_test2.npy\n",
      "0.2\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "from scipy.special import softmax\n",
    "\n",
    "# test = pd.read_parquet(\"/kaggle/input/llmse-zeroshot-gettopk/test.pq\")\n",
    "test = pd.read_parquet(\"test.pq\")\n",
    "curr_scores = []\n",
    "\n",
    "for f in glob.glob(\"scores_*.npy\"):\n",
    "    print(f)\n",
    "    a = np.array(np.load(f))\n",
    "    a = softmax(a.reshape(-1,5), axis=1)\n",
    "    a = a.flatten()\n",
    "    print(a.mean(axis=0))\n",
    "    curr_scores.append(a)\n",
    "    # os.remove(f)\n",
    "curr_scores = np.array(curr_scores)\n",
    "preds = np.nanmean(curr_scores, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6d6271a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T04:21:42.403651Z",
     "iopub.status.busy": "2024-12-04T04:21:42.403239Z",
     "iopub.status.idle": "2024-12-04T04:21:42.414556Z",
     "shell.execute_reply": "2024-12-04T04:21:42.413752Z"
    },
    "papermill": {
     "duration": 0.036106,
     "end_time": "2024-12-04T04:21:42.416209",
     "exception": false,
     "start_time": "2024-12-04T04:21:42.380103",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "options = \"ABCDE\"\n",
    "indices = list(range(5))\n",
    "\n",
    "option_to_index = {option: index for option, index in zip(options, indices)}\n",
    "index_to_option = {index: option for option, index in zip(options, indices)}\n",
    "post_proc_preds = []\n",
    "\n",
    "for gr in range(0, len(preds), 5):\n",
    "    pr = preds[gr : gr + 5]\n",
    "    pr = np.argsort(-pr)[:3]\n",
    "    post_proc_preds.append(\" \".join([index_to_option[x] for x in pr]))\n",
    "len(post_proc_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11d16445",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T04:21:42.463493Z",
     "iopub.status.busy": "2024-12-04T04:21:42.463232Z",
     "iopub.status.idle": "2024-12-04T04:21:42.468119Z",
     "shell.execute_reply": "2024-12-04T04:21:42.467460Z"
    },
    "papermill": {
     "duration": 0.029643,
     "end_time": "2024-12-04T04:21:42.469515",
     "exception": false,
     "start_time": "2024-12-04T04:21:42.439872",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test[\"prediction\"] = post_proc_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a4622ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T04:21:42.515775Z",
     "iopub.status.busy": "2024-12-04T04:21:42.515245Z",
     "iopub.status.idle": "2024-12-04T04:21:42.538755Z",
     "shell.execute_reply": "2024-12-04T04:21:42.537946Z"
    },
    "papermill": {
     "duration": 0.048316,
     "end_time": "2024-12-04T04:21:42.540363",
     "exception": false,
     "start_time": "2024-12-04T04:21:42.492047",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>D E A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>E A B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>E C A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>A C E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>A D B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>195</td>\n",
       "      <td>B D E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>196</td>\n",
       "      <td>C B A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>197</td>\n",
       "      <td>B D C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>198</td>\n",
       "      <td>E C B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>199</td>\n",
       "      <td>D C B</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id prediction\n",
       "0      0      D E A\n",
       "1      1      E A B\n",
       "2      2      E C A\n",
       "3      3      A C E\n",
       "4      4      A D B\n",
       "..   ...        ...\n",
       "195  195      B D E\n",
       "196  196      C B A\n",
       "197  197      B D C\n",
       "198  198      E C B\n",
       "199  199      D C B\n",
       "\n",
       "[200 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[[\"id\", \"prediction\"]].to_csv(\"submission.csv\", index=False)\n",
    "pd.read_csv(\"submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 6169864,
     "sourceId": 54662,
     "sourceType": "competition"
    },
    {
     "sourceId": 141335285,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 141652465,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 142004340,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 142017888,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 142017966,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 142018053,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 142021399,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 142022570,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 142022613,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 144334653,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 144343082,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 144343125,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 144349788,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 144349842,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 144349880,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 144349949,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 144349992,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 144359830,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 144359877,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 144359928,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 144359980,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 145938933,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 210083950,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 210094885,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 210766779,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2176.495444,
   "end_time": "2024-12-04T04:21:43.381492",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-04T03:45:26.886048",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
